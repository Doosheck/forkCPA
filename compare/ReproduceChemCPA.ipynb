{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79abcfd3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from seml.config import generate_configs, read_config\n",
    "from chemCPA.experiments_run import ExperimentWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "768ee7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare variables\n",
    "train_split = 0.8\n",
    "rng = np.random.default_rng(1337)\n",
    "ood = [\"Dacinostat\", \"Givinostat\", \"Belinostat\", \"Hesperadin\", \"Quisinostat\", \"Alvespimycin\", \"Tanespimycin\", \"TAK-901\", \"Flavopiridol\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67295837",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/dss/dsshome1/0A/di93hoq/forkCPA/compare'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0089c6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "adata = sc.read_h5ad(\"/dss/dsshome1/0A/di93hoq/ConditionalMongeGap/Datasets/sciplex_complete_middle_subset.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cefbaa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new split to the dataset to be used as a comparisson to Conditional Monge Gap\n",
    "adata.obs[\"compare_split\"] = np.where(\n",
    "    adata.obs[\"condition\"].isin(ood),\n",
    "    \"ood\",\n",
    "    rng.choice([\"training\", \"test\"], p=[train_split, 1-train_split]),\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46ebece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.subsample(\n",
    "    adata,\n",
    "    n_obs=5000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89478fa4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ood', 'train', 'test']\n",
       "Categories (3, object): ['ood', 'test', 'train']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obs[\"split_random\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b72e4ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset\n",
    "adata.write_h5ad(\"/dss/dsshome1/0A/di93hoq/ConditionalMongeGap/Datasets/sciplex_complete_middle_subset_compare.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "284e99d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/0A/di93hoq/forkCPA\n"
     ]
    }
   ],
   "source": [
    "cd /dss/dsshome1/0A/di93hoq/forkCPA/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2823705",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'data_params': {'covariate_keys': 'cell_type',\n",
      "                             'dataset_path': '/dss/dsshome1/0A/di93hoq/ConditionalMongeGap/Datasets/sciplex_complete_middle_subset_compare.h5ad',\n",
      "                             'degs_key': 'all_DEGs',\n",
      "                             'dose_key': 'dose',\n",
      "                             'pert_category': 'cov_drug_dose_name',\n",
      "                             'perturbation_key': 'condition',\n",
      "                             'smiles_key': 'SMILES',\n",
      "                             'split_key': 'split_random',\n",
      "                             'use_drugs_idx': True},\n",
      "             'dataset_type': 'trapnell'},\n",
      " 'model': {'additional_params': {'decoder_activation': 'ReLU',\n",
      "                                 'doser_type': 'amortized',\n",
      "                                 'multi_task': False,\n",
      "                                 'patience': 50,\n",
      "                                 'seed': 1337},\n",
      "           'append_ae_layer': True,\n",
      "           'embedding': {'directory': 'embeddings', 'model': 'rdkit'},\n",
      "           'enable_cpa_mode': True,\n",
      "           'hparams': {'adversary_depth': 4,\n",
      "                       'adversary_lr': 0.00036374585440139074,\n",
      "                       'adversary_steps': 3,\n",
      "                       'adversary_wd': 7.459343285726542e-07,\n",
      "                       'adversary_width': 128,\n",
      "                       'autoencoder_depth': 4,\n",
      "                       'autoencoder_lr': 0.0005611516415334506,\n",
      "                       'autoencoder_wd': 1.329291894316214e-07,\n",
      "                       'autoencoder_width': 256,\n",
      "                       'batch_size': 128,\n",
      "                       'dim': 194,\n",
      "                       'dosers_depth': 3,\n",
      "                       'dosers_lr': 0.0005611516415334506,\n",
      "                       'dosers_wd': 1.329291894316214e-07,\n",
      "                       'dosers_width': 64,\n",
      "                       'dropout': 0.262378,\n",
      "                       'embedding_encoder_depth': 4,\n",
      "                       'embedding_encoder_width': 128,\n",
      "                       'penalty_adversary': 0.19906996673933372,\n",
      "                       'reg_adversary': 3.9813993347866736,\n",
      "                       'reg_adversary_cov': 11.844319751820388,\n",
      "                       'reg_multi_task': 0,\n",
      "                       'step_size_lr': 50},\n",
      "           'load_pretrained': False,\n",
      "           'pretrained_model_hashes': None,\n",
      "           'pretrained_model_path': 'checkpoints'},\n",
      " 'profiling': {'outdir': './', 'run_profiler': False},\n",
      " 'training': {'checkpoint_freq': 2,\n",
      "              'full_eval_during_train': False,\n",
      "              'max_minutes': 5,\n",
      "              'num_epochs': 5,\n",
      "              'run_eval_disentangle': True,\n",
      "              'run_eval_logfold': False,\n",
      "              'run_eval_r2': True,\n",
      "              'run_eval_r2_sc': False,\n",
      "              'save_checkpoints': True,\n",
      "              'save_dir': 'compare/checkpoints'}}\n"
     ]
    }
   ],
   "source": [
    "exp = ExperimentWrapper(init_all=False)\n",
    "# this is how seml loads the config file internally\n",
    "assert Path(\n",
    "    \"manual_run.yaml\"\n",
    ").exists(), \"config file not found\"\n",
    "seml_config, slurm_config, experiment_config = read_config(\n",
    "   \"manual_run.yaml\"\n",
    ")\n",
    "# we take the first config generated\n",
    "configs = generate_configs(experiment_config)\n",
    "if len(configs) > 1:\n",
    "    print(\"Careful, more than one config generated from the yaml file\")\n",
    "args = configs[0]\n",
    "pprint(args)\n",
    "\n",
    "exp.seed = 1337\n",
    "# loads the dataset splits\n",
    "exp.init_dataset(**args[\"dataset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81b21947",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exp.init_drug_embedding(embedding=args[\"model\"][\"embedding\"])\n",
    "exp.init_model(\n",
    "    hparams=args[\"model\"][\"hparams\"],\n",
    "    additional_params=args[\"model\"][\"additional_params\"],\n",
    "    load_pretrained=args[\"model\"][\"load_pretrained\"],\n",
    "    append_ae_layer=args[\"model\"][\"append_ae_layer\"],\n",
    "    enable_cpa_mode=args[\"model\"][\"enable_cpa_mode\"],\n",
    "    pretrained_model_path=args[\"model\"][\"pretrained_model_path\"],\n",
    "    pretrained_model_hashes=args[\"model\"][\"pretrained_model_hashes\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ba2540a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['training', 'training_control', 'training_treated', 'test', 'test_control', 'test_treated', 'ood'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8b07589",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# setup the torch DataLoader\n",
    "exp.update_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64069c5a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /dss/dsshome1/0A/di93hoq/forkCPA\n",
      "Save dir: compare/checkpoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Early stopping score was None!\n",
      "WARNING:root:Early stopping score was None!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of disentanglement testdata: 731\n",
      "\n",
      "Took 0.4 min for evaluation.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'epoch': [0, 1, 2, 3, 4],\n",
       " 'stats_epoch': [4],\n",
       " 'loss_reconstruction': [-9.099171981215477,\n",
       "  -24.392128109931946,\n",
       "  -38.112823724746704,\n",
       "  -48.48217499256134,\n",
       "  -58.22065711021423],\n",
       " 'loss_adv_drugs': [148.400963306427,\n",
       "  148.87963104248047,\n",
       "  148.8683567047119,\n",
       "  148.32721185684204,\n",
       "  148.072283744812],\n",
       " 'loss_adv_covariates': [38.79986619949341,\n",
       "  36.247304916381836,\n",
       "  32.05805975198746,\n",
       "  38.10633182525635,\n",
       "  45.02868127822876],\n",
       " 'penalty_adv_drugs': [4.205532506108284,\n",
       "  0.7807078063488007,\n",
       "  0.17748189065605402,\n",
       "  0.10121615417301655,\n",
       "  0.07536241970956326],\n",
       " 'penalty_adv_covariates': [0.11973072402179241,\n",
       "  0.08446761826053262,\n",
       "  0.06932178698480129,\n",
       "  0.07250320492312312,\n",
       "  0.04423150490038097],\n",
       " 'loss_multi_task': [0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'elapsed_time_min': 0.10051395495732625,\n",
       " 'perturbation disentanglement': [1.0],\n",
       " 'optimal for perturbations': [0.03967168262653899],\n",
       " 'covariate disentanglement': [[1.0]],\n",
       " 'optimal for covariates': [[0.49931600689888]],\n",
       " 'training': [[0.6408230647808169,\n",
       "   0.7413998086278032,\n",
       "   0.39937439342824427,\n",
       "   0.43277257244761397]],\n",
       " 'test': [[]],\n",
       " 'ood': [[]],\n",
       " 'total_epochs': 4}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.train(**args[\"training\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d265816c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_4.pt\r\n"
     ]
    }
   ],
   "source": [
    "!ls compare/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478e6c97",
   "metadata": {},
   "source": [
    "# Model loading and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b516fd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/dss/dsshome1/0A/di93hoq/forkCPA'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bda03ba0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from notebooks.utils import load_config, load_dataset, load_model, load_smiles, compute_pred\n",
    "from chemCPA.train import evaluate, compute_prediction, repeat_n\n",
    "import torch\n",
    "from chemCPA.model import ComPert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36a1f03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seml_collection = \"multi_task\"\n",
    "model_hash_pretrained_rdkit = \"model_4\"  # Fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc53a487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_drugs_name_to_idx',\n",
       " 'atomic_Ñovars_dict',\n",
       " 'canon_smiles_unique_sorted',\n",
       " 'covariate_keys',\n",
       " 'covariate_names',\n",
       " 'covariate_names_unique',\n",
       " 'covariates',\n",
       " 'ctrl',\n",
       " 'ctrl_name',\n",
       " 'de_genes',\n",
       " 'degs',\n",
       " 'dosages',\n",
       " 'dose_key',\n",
       " 'dose_names',\n",
       " 'drug_name_to_idx',\n",
       " 'drugs_idx',\n",
       " 'drugs_names',\n",
       " 'drugs_names_unique_sorted',\n",
       " 'genes',\n",
       " 'indices',\n",
       " 'max_num_perturbations',\n",
       " 'num_covariates',\n",
       " 'num_drugs',\n",
       " 'num_genes',\n",
       " 'pert_categories',\n",
       " 'perturbation_key',\n",
       " 'smiles_key',\n",
       " 'subset',\n",
       " 'use_drugs_idx',\n",
       " 'var_names']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(exp.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "953ee6ea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'autoencoder',\n",
       " 'dataset',\n",
       " 'datasets',\n",
       " 'drug_embeddings',\n",
       " 'embedding_model_type',\n",
       " 'init_all',\n",
       " 'init_dataset',\n",
       " 'init_drug_embedding',\n",
       " 'init_model',\n",
       " 'load_state_dict',\n",
       " 'seed',\n",
       " 'train',\n",
       " 'update_datasets']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17806d74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = torch.load(\"compare/checkpoints/model_4.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48753340",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(\n",
    "    state_dict,\n",
    "    cov_adv_state_dicts,\n",
    "    cov_emb_state_dicts,\n",
    "    init_args,\n",
    "    history,\n",
    "        ) = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6f7cd40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = ComPert(\n",
    "        **init_args, drug_embeddings=exp.drug_embeddings, append_layer_width=256\n",
    ")\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "678908cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A549', 'K562', 'MCF7']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "992it [00:00, 41382.79it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (7x2000 and 256x2000)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompute_pred\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/forkCPA/notebooks/utils.py:267\u001b[0m, in \u001b[0;36mcompute_pred\u001b[0;34m(model, dataset, dosages, cell_lines, genes_control, use_DEGs, verbose)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# copies just the needed genes to GPU\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# Could try moving the whole genes tensor to GPU once for further speedups (but more memory problems)\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m genes_control \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;66;03m# print(\"Predicting AE alike.\")\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m     mean_pred, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_prediction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43memb_drugs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43memb_covs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;66;03m# print(\"Predicting counterfactuals.\")\u001b[39;00m\n\u001b[1;32m    275\u001b[0m     mean_pred, _ \u001b[38;5;241m=\u001b[39m compute_prediction(\n\u001b[1;32m    276\u001b[0m         model,\n\u001b[1;32m    277\u001b[0m         genes_control,\n\u001b[1;32m    278\u001b[0m         emb_drugs,\n\u001b[1;32m    279\u001b[0m         emb_covs,\n\u001b[1;32m    280\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/chemCPA/train.py:46\u001b[0m, in \u001b[0;36mcompute_prediction\u001b[0;34m(autoencoder, genes, emb_drugs, emb_covs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m autoencoder\u001b[38;5;241m.\u001b[39muse_drugs_idx:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(emb_drugs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m---> 46\u001b[0m     genes_pred \u001b[38;5;241m=\u001b[39m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgenes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrugs_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memb_drugs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdosages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memb_drugs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcovariates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memb_covs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     genes_pred \u001b[38;5;241m=\u001b[39m autoencoder\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m     54\u001b[0m         genes\u001b[38;5;241m=\u001b[39mgenes, drugs\u001b[38;5;241m=\u001b[39memb_drugs, covariates\u001b[38;5;241m=\u001b[39memb_covs\n\u001b[1;32m     55\u001b[0m     )[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/chemCPA/model.py:660\u001b[0m, in \u001b[0;36mComPert.predict\u001b[0;34m(self, genes, drugs, drugs_idx, dosages, covariates, return_latent_basal)\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (drugs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (drugs_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dosages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    656\u001b[0m genes, drugs, drugs_idx, dosages, covariates \u001b[38;5;241m=\u001b[39m _move_inputs(\n\u001b[1;32m    657\u001b[0m     genes, drugs, drugs_idx, dosages, covariates, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    658\u001b[0m )\n\u001b[0;32m--> 660\u001b[0m latent_basal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    662\u001b[0m latent_treated \u001b[38;5;241m=\u001b[39m latent_basal\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_drugs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/chemCPA/model.py:218\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    216\u001b[0m     dim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x[:, :dim]), x[:, dim:]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (7x2000 and 256x2000)"
     ]
    }
   ],
   "source": [
    "compute_pred(\n",
    "    model,\n",
    "    exp.datasets[\"training\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d5fc308",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ComPert(exp.dataset.num_genes, exp.dataset.num_drugs, exp.dataset.num_covariates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df8b7a97",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ComPert:\n\tMissing key(s) in state_dict: \"drug_embedding_encoder.network.0.weight\", \"drug_embedding_encoder.network.0.bias\", \"dosers.beta\", \"dosers.bias\". \n\tUnexpected key(s) in state_dict: \"adversary_drugs.network.10.weight\", \"adversary_drugs.network.10.bias\", \"adversary_drugs.network.10.running_mean\", \"adversary_drugs.network.10.running_var\", \"adversary_drugs.network.10.num_batches_tracked\", \"adversary_drugs.network.12.weight\", \"adversary_drugs.network.12.bias\", \"dosers.network.0.weight\", \"dosers.network.0.bias\", \"dosers.network.1.weight\", \"dosers.network.1.bias\", \"dosers.network.1.running_mean\", \"dosers.network.1.running_var\", \"dosers.network.1.num_batches_tracked\", \"dosers.network.3.weight\", \"dosers.network.3.bias\", \"dosers.network.4.weight\", \"dosers.network.4.bias\", \"dosers.network.4.running_mean\", \"dosers.network.4.running_var\", \"dosers.network.4.num_batches_tracked\", \"dosers.network.6.weight\", \"dosers.network.6.bias\", \"dosers.network.7.weight\", \"dosers.network.7.bias\", \"dosers.network.7.running_mean\", \"dosers.network.7.running_var\", \"dosers.network.7.num_batches_tracked\", \"dosers.network.9.weight\", \"dosers.network.9.bias\". \n\tsize mismatch for encoder.network.0.weight: copying a param with shape torch.Size([256, 2000]) from checkpoint, the shape in current model is torch.Size([512, 2000]).\n\tsize mismatch for encoder.network.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.1.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.1.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.3.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for encoder.network.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.4.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.4.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.4.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.4.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.6.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for encoder.network.6.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.7.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.7.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.7.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.7.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.9.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for encoder.network.9.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.10.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.10.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.10.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.10.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.12.weight: copying a param with shape torch.Size([194, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for encoder.network.12.bias: copying a param with shape torch.Size([194]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.network.0.weight: copying a param with shape torch.Size([256, 194]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for decoder.network.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.1.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.1.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.3.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for decoder.network.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.4.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.4.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.4.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.4.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.6.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for decoder.network.6.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.7.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.7.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.7.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.7.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.9.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for decoder.network.9.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.10.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.10.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.10.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.10.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.12.weight: copying a param with shape torch.Size([4000, 256]) from checkpoint, the shape in current model is torch.Size([4000, 512]).\n\tsize mismatch for adversary_drugs.network.0.weight: copying a param with shape torch.Size([128, 194]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for adversary_drugs.network.9.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([188, 128]).\n\tsize mismatch for adversary_drugs.network.9.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([188]).\n\tsize mismatch for drug_embeddings.weight: copying a param with shape torch.Size([188, 194]) from checkpoint, the shape in current model is torch.Size([188, 256]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompare/checkpoints/model_4.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ComPert:\n\tMissing key(s) in state_dict: \"drug_embedding_encoder.network.0.weight\", \"drug_embedding_encoder.network.0.bias\", \"dosers.beta\", \"dosers.bias\". \n\tUnexpected key(s) in state_dict: \"adversary_drugs.network.10.weight\", \"adversary_drugs.network.10.bias\", \"adversary_drugs.network.10.running_mean\", \"adversary_drugs.network.10.running_var\", \"adversary_drugs.network.10.num_batches_tracked\", \"adversary_drugs.network.12.weight\", \"adversary_drugs.network.12.bias\", \"dosers.network.0.weight\", \"dosers.network.0.bias\", \"dosers.network.1.weight\", \"dosers.network.1.bias\", \"dosers.network.1.running_mean\", \"dosers.network.1.running_var\", \"dosers.network.1.num_batches_tracked\", \"dosers.network.3.weight\", \"dosers.network.3.bias\", \"dosers.network.4.weight\", \"dosers.network.4.bias\", \"dosers.network.4.running_mean\", \"dosers.network.4.running_var\", \"dosers.network.4.num_batches_tracked\", \"dosers.network.6.weight\", \"dosers.network.6.bias\", \"dosers.network.7.weight\", \"dosers.network.7.bias\", \"dosers.network.7.running_mean\", \"dosers.network.7.running_var\", \"dosers.network.7.num_batches_tracked\", \"dosers.network.9.weight\", \"dosers.network.9.bias\". \n\tsize mismatch for encoder.network.0.weight: copying a param with shape torch.Size([256, 2000]) from checkpoint, the shape in current model is torch.Size([512, 2000]).\n\tsize mismatch for encoder.network.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.1.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.1.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.3.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for encoder.network.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.4.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.4.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.4.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.4.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.6.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for encoder.network.6.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.7.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.7.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.7.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.7.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.9.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for encoder.network.9.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.10.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.10.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.10.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.10.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for encoder.network.12.weight: copying a param with shape torch.Size([194, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for encoder.network.12.bias: copying a param with shape torch.Size([194]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.network.0.weight: copying a param with shape torch.Size([256, 194]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for decoder.network.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.1.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.1.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.3.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for decoder.network.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.4.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.4.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.4.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.4.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.6.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for decoder.network.6.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.7.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.7.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.7.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.7.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.9.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for decoder.network.9.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.10.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.10.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.10.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.10.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.network.12.weight: copying a param with shape torch.Size([4000, 256]) from checkpoint, the shape in current model is torch.Size([4000, 512]).\n\tsize mismatch for adversary_drugs.network.0.weight: copying a param with shape torch.Size([128, 194]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for adversary_drugs.network.9.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([188, 128]).\n\tsize mismatch for adversary_drugs.network.9.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([188]).\n\tsize mismatch for drug_embeddings.weight: copying a param with shape torch.Size([188, 194]) from checkpoint, the shape in current model is torch.Size([188, 256])."
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"compare/checkpoints/model_4.pt\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d590b81b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "['drug_embeddings.weight', 'drug_embedding_encoder.network.0.weight', 'drug_embedding_encoder.network.0.bias', 'drug_embedding_encoder.network.1.weight', 'drug_embedding_encoder.network.1.bias', 'drug_embedding_encoder.network.1.running_mean', 'drug_embedding_encoder.network.1.running_var', 'drug_embedding_encoder.network.3.weight', 'drug_embedding_encoder.network.3.bias', 'drug_embedding_encoder.network.4.weight', 'drug_embedding_encoder.network.4.bias', 'drug_embedding_encoder.network.4.running_mean', 'drug_embedding_encoder.network.4.running_var', 'drug_embedding_encoder.network.6.weight', 'drug_embedding_encoder.network.6.bias', 'drug_embedding_encoder.network.7.weight', 'drug_embedding_encoder.network.7.bias', 'drug_embedding_encoder.network.7.running_mean', 'drug_embedding_encoder.network.7.running_var', 'drug_embedding_encoder.network.9.weight', 'drug_embedding_encoder.network.9.bias', 'drug_embedding_encoder.network.10.weight', 'drug_embedding_encoder.network.10.bias', 'drug_embedding_encoder.network.10.running_mean', 'drug_embedding_encoder.network.10.running_var', 'drug_embedding_encoder.network.12.weight', 'drug_embedding_encoder.network.12.bias']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m configs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      2\u001b[0m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanon_smiles_unique_sorted\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompare/checkpoints/model_4.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/forkCPA/notebooks/utils.py:147\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(config, canon_smiles_unique_sorted, model_checkp)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# make sure we didn't accidentally load the embedding from the state_dict\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     torch\u001b[38;5;241m.\u001b[39mtesting\u001b[38;5;241m.\u001b[39massert_allclose(model\u001b[38;5;241m.\u001b[39mdrug_embeddings\u001b[38;5;241m.\u001b[39mweight, embedding\u001b[38;5;241m.\u001b[39mweight)\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28mlen\u001b[39m(incomp_keys\u001b[38;5;241m.\u001b[39mmissing_keys) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrug_embeddings.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m incomp_keys\u001b[38;5;241m.\u001b[39mmissing_keys\n\u001b[1;32m    150\u001b[0m     ), incomp_keys\u001b[38;5;241m.\u001b[39mmissing_keys\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# assert len(incomp_keys.unexpected_keys) == 0, incomp_keys.unexpected_keys\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, embedding\n",
      "\u001b[0;31mAssertionError\u001b[0m: ['drug_embeddings.weight', 'drug_embedding_encoder.network.0.weight', 'drug_embedding_encoder.network.0.bias', 'drug_embedding_encoder.network.1.weight', 'drug_embedding_encoder.network.1.bias', 'drug_embedding_encoder.network.1.running_mean', 'drug_embedding_encoder.network.1.running_var', 'drug_embedding_encoder.network.3.weight', 'drug_embedding_encoder.network.3.bias', 'drug_embedding_encoder.network.4.weight', 'drug_embedding_encoder.network.4.bias', 'drug_embedding_encoder.network.4.running_mean', 'drug_embedding_encoder.network.4.running_var', 'drug_embedding_encoder.network.6.weight', 'drug_embedding_encoder.network.6.bias', 'drug_embedding_encoder.network.7.weight', 'drug_embedding_encoder.network.7.bias', 'drug_embedding_encoder.network.7.running_mean', 'drug_embedding_encoder.network.7.running_var', 'drug_embedding_encoder.network.9.weight', 'drug_embedding_encoder.network.9.bias', 'drug_embedding_encoder.network.10.weight', 'drug_embedding_encoder.network.10.bias', 'drug_embedding_encoder.network.10.running_mean', 'drug_embedding_encoder.network.10.running_var', 'drug_embedding_encoder.network.12.weight', 'drug_embedding_encoder.network.12.bias']"
     ]
    }
   ],
   "source": [
    "config = configs[0]\n",
    "config[\"config_hash\"] = \"model_4\"\n",
    "load_model(config, exp.dataset.canon_smiles_unique_sorted,\"compare/checkpoints/model_4.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40b11d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
